import json
import os
import time

import pandas as pd
import requests
from dotenv import load_dotenv

load_dotenv()
token = os.getenv("GITHUB_TOKEN")

if not token:
    raise ValueError("Erro: O token do GitHub n√£o foi encontrado. Verifique o arquivo .env.")

GITHUB_GRAPHQL_URL = "https://api.github.com/graphql"

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json"
}

def fetchRepositories():
    """Faz a requisi√ß√£o GraphQL com pagina√ß√£o para obter 100 reposit√≥rios em 4 chamadas de 25."""
    allRepos = []
    cursor = None
    totalRepos = 1000  # N√∫mero total de reposit√≥rios desejado
    batchSize = 25  # Reposit√≥rios por chamada
    numBatches = totalRepos // batchSize  # Total de chamadas necess√°rias

    for batch in range(numBatches):
        print(f"üîÑ Buscando reposit√≥rios... (Chamada {batch + 1}/{numBatches})")

        query = f"""
        {{
          search(query: "stars:>10000", type: REPOSITORY, first: {batchSize}, after: {json.dumps(cursor) if cursor else "null"}) {{
            edges {{
              node {{
                ... on Repository {{
                  name
                  owner {{ login }}
                  createdAt
                  updatedAt
                  stargazerCount
                  primaryLanguage {{ name }}
                  pullRequests(states: MERGED) {{ totalCount }}
                  releases {{ totalCount }}
                  issues {{ totalCount }}
                }}
              }}
            }}
            pageInfo {{
              hasNextPage
              endCursor
            }}
          }}
        }}
        """

        for attempt in range(3):
            response = requests.post(GITHUB_GRAPHQL_URL, json={"query": query}, headers=headers, )

            if response.status_code == 200:
                data = response.json()
                repositories = data['data']['search']['edges']

                if not repositories:
                    print("‚ö†Ô∏è Nenhum reposit√≥rio encontrado.")
                    return None

                allRepos.extend(repositories)

                # Atualizar cursor para a pr√≥xima p√°gina
                pageInfo = data['data']['search']['pageInfo']
                cursor = pageInfo["endCursor"] if pageInfo["hasNextPage"] else None

                print(f"‚úÖ Chamada {batch + 1}/{numBatches} conclu√≠da com sucesso! ({len(allRepos)}/{totalRepos} reposit√≥rios coletados)\n")
                break

            else:
                print(f"‚ö†Ô∏è Erro {response.status_code}: {response.text}. Tentativa {attempt + 1}/3...")
                time.sleep(5)

    return allRepos

def processData(repositories):
    """Processa os dados da API para um DataFrame."""
    repoList = []

    for repo in repositories:
        node = repo['node']
        repoList.append({
            "Nome": node['name'],
            "Propriet√°rio": node['owner']['login'],
            "Data de Cria√ß√£o": node['createdAt'],
            "√öltima Atualiza√ß√£o": node['updatedAt'],
            "Estrelas": node['stargazerCount'],
            "Pull Requests Aceitos": node['pullRequests']['totalCount'],
            "Releases": node['releases']['totalCount'],
            "Total de Issues": node['issues']['totalCount'],
            "Linguagem Principal": node['primaryLanguage']['name'] if node['primaryLanguage'] else "Desconhecido"
        })

    return pd.DataFrame(repoList)

def csv_writer(data, filename):
    dataframe = pd.DataFrame(data)

    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        csvfile.write(dataframe.to_string(index=False))

# Run
repositories = fetchRepositories()
if repositories:
    df = processData(repositories)
    if df is not None:
        pd.set_option('display.max_rows', None)
        pd.set_option('display.max_columns', None)
        csv_writer(df.to_dict('records'), 'repos.csv')
        print(df.to_string())
